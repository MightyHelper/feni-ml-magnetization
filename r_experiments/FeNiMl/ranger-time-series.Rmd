---
title: "Random Forests: Sirve para todo?"
output: html_notebook
date: '2023-04-27'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Objetivo

Predecir el consumo para los 3 meses siguiente dada una serie de datos del consumo previo junto a variables exogenas. Las variables `next_consume`, `next_2_consume` y `next_3_consume` son las variables dependientes que queremos predecir.

## Cargar Bibliotecas

```{r message=FALSE, warning=FALSE}
install.packages("readr")
install.packages("ranger")
install.packages("dplyr")
install.packages("skimr")
install.packages("caret")
library(readr) # para leeer el dataset
library(ranger) # random forest con esteroides
library(dplyr) # para manipular datos
library(skimr) # para mirar los datos
library(caret) # framework de machine learning
```

## Cargar el dataset

```{r message=FALSE, warning=FALSE, include=FALSE}
dataset <- readr::read_csv("https://www.dropbox.com/s/k1hg9pm975ouwl0/dataraw_ai.csv?dl=1")
dataset <- dataset %>% mutate(Date= lubridate::ym(Date))
dataset <- dataset %>% select(-Agr.Item,-Date, -prev_date, -`...1`)
dataset
```

```{r}
dataset %>% select(Date)
dataset %>% names() %>% as.data.frame()
```

```{r}
 skimr::skim(dataset)# %>% knitr::kable() %>% kable_styling(font_size = 9)
```

## La Metodologia

![](https://harpomaxx.github.io/post/2020-09-09-experimental-design.en_files/ml-experimenta-designA.png){style="color:white"}

```{r}

dataset <- dataset %>% tidyr::drop_na()
  # mirando los mismos datos, predicen parecido para los proximos 3 meses.
train<-dataset %>% sample_frac(0.8)
test <-setdiff(dataset,train)
train
test
```

## Modelo para predecir next_consume

```{r}
rf_model1 <- ranger(next_consume ~ . -next_2_consume - next_3_consume - trimestral_consume ,data=train)
rf_model1
```

```{r}
rf_model1$prediction.error
```

### Entrenamiento

#### Out Of Box Sampling.

Los errores MSE y R squared se calculan sobre el OOB. El concepto de **OOB** está relacionado con el proceso de **bootstrapping**, que es una técnica de muestreo utilizada en la construcción de los árboles de decisión en Random Forest. En bootstrapping, se extrae una muestra aleatoria de los datos de entrenamiento con reemplazo, lo que significa que algunas instancias pueden ser elegidas varias veces, mientras que otras pueden no ser elegidas en absoluto.\
\

#### Importancia de las variables

```{r}
rf_model1 <- ranger(next_consume ~ . -next_2_consume - next_3_consume - trimestral_consume ,data=train, importance = "impurity")
rf_model1
```

**impurity**: Este es el método predeterminado, que calcula la importancia de una característica basándose en la disminución de la impureza del nodo (por ejemplo, Gini o entropía) cuando una característica se utiliza para dividir en los árboles de decisión. Cuanto **mayor sea la disminución de la impureza, más importante se considera la característica**.

```{r}

rf_model1$variable.importance
data.frame(impurity=rf_model1$variable.importance) %>% arrange(desc(impurity))

```

```{r}
rf_model1 <- ranger(next_consume ~ prev_6_consume + month_consume + dolar_estadounidense -next_2_consume - next_3_consume - trimestral_consume ,data=train, importance = "impurity")
rf_model1
```

### Prediccion

```{r}
predictions1 <- predict(rf_model1,data = test, type='response')
predictions1$predictions %>% as.data.frame
```

#### Error MSE en test

```{r}
mse<-function(act,pred) {mean((act- pred)^2)}

data.frame(pred=predictions1$predictions, act=test$next_consume) %>% summarise(mse=mse(act,pred))
```

#### Intervalos de prediccion (quantile regression)

En vez de utilizar el promedio se utilzan los cuantiles para tener un intervalo de predicción (Meinshausen, 2006). A la hora de realizar el split, en vez de utilizar MSE o alguna otra metrica de impureza, se utiliza una metrica que tiene en cuenta a los cuantiles . Luego en cada hoja en vez de calcular el promedio, se calculan cuantiles.

```{r}
rf_model1 <- ranger(next_consume ~ . -next_2_consume - next_3_consume - trimestral_consume ,data=train, importance = "impurity",quantreg = TRUE)
rf_model1
```

```{r}
predictions1 <- predict(rf_model1,data = test, type= "quantiles")
predictions1$predictions %>% as.data.frame()


```

```{r}
p1<-data.frame(predictions1$predictions,act=test$next_consume,label="FCT 1M")

p1  %>% ggplot()+
  geom_point(aes(x=act,y=quantile..0.5),color='red')+
  geom_errorbar(aes(x=act,y=quantile..0.5,ymax=quantile..0.9,ymin=quantile..0.1),color='orange')+
  theme_classic()
```

#### Intervalo de confianza

Similar al intervalo de prediccion. En las implementaciones de Random Forests, suele confudirse. En terminos generales, uno se aplica sobre una observacion/predicción en general, mientras que el otro trata sobre estadisticos. Un ejemplo seria, la diferencia entre desviacion estandar de una variable y el error estandar sobre un conjunto de muestras.

If we change the training dataset just a little bit, will Random Forest give you the same result for that particular example?

(Wagner et al. 2014) Basado en una tecnica que se llama jacknife.

```{r}
rf_model1 <- ranger(next_consume ~ stock + month_consume + prev_6_consume + prev_2_consume  ,data=train, importance = "impurity",keep.inbag  = TRUE)
rf_model1
```

```{r}
predictions1 <- predict(rf_model1,data = train, type= "se")

predictions1$predictions %>% as.data.frame()
predictions1$se %>% as.data.frame()

data.frame(pred=predictions1$predictions, se= predictions1$se, act=train$next_consume) %>% ggplot()+
  geom_point(aes(x=act,y=pred),color='red')+
  geom_errorbar(aes(x=act,y=pred,ymax=pred+se,ymin=pred-se),color='orange')+
  theme_classic()
```
